# 回顾与分类
1. 离散问题一般是分类问题，连续问题一般是回归问题。
2. 对于分类问题,某个区域x值会映射到y值上；对于回归问题，一个x值会映射到一个y值

# 线性回归
1. 使用一个直线方程描述数据之间的关系

# 逻辑回归
1. 是一个分类算法而非回归算法,属于广义的线性回归模型;
2. 一般映射到sigmoid函数后，大于或者小于0.5可以用于去做不同的决策

# 决策树
1. 分为分类数和决策树,前者数据定性，后者输出定量
2. 决策树的构造过程：
    - 特征选择
    - 决策树生成:通过信息增益(ID3算法),信息增益比(ID4.5)构造决策树,决定哪个是根节点，哪个是中间节点
    - 决策树剪枝:防止过拟合,可能一棵树只适合一个数据集，不能适合所有数据样本,主要目的还是为了增强泛化能力

3. 分类回归树（CART）:
    - 回归树生成:最小二乘回归树生成算法
    - 分类树生成：基尼指数算法

4. 集成学习
    - Boosting：将弱学习的分类器提升为强学习的分类器,来提高预测精度,典型为AdaBoost,GBDT也是Boosting成员之一
    - Bagging:通过自助采样的方法生成多种并行的分类器。通过少数服从多数来决定最终结果，典型代表为随机森林

5. 随机森林
    - 思想:单颗树是决策树，多棵是随机森林，解决决策树泛化能力弱的问题
    - 每棵树由有放回的从样本集中抽取子集合训练得到

6. GBDT
    - GBDT是回归树，不是分类树
    - 核心是累加所有树的计算结果作为最终结果，可以理解为连续的结果
    - 利用损失函数的负梯度去模拟残差，对于一般的损失函数，只要求一阶导就可以了
    - 本质实际上是不断的去拟合残差的一个过程，会不断的用残差去构建一颗树

7. 几种Boost的基本思想
    - Adaboost: 提高前一轮弱分类错误样本权重，降低正确分类样本权重
    - GBDT:
    利用损失函数的负梯度去模拟残差，对于一般的损失函数，只要求一阶导就可以了
    - XGBoost对GBDT的代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数

# 神经网络
8. 神经网络类别
    - 单层神经网络（感知机）：类似逻辑回归，只能做线性分类
    - 两层神经网络：带有一隐层，可以做非线性分类了
    - 多层神经网络：深度学习
    - 卷积神经网络(CNN)：非全连接
    - 循环神经网络(RNN)：处理序列数据

9. 神经网络中的三种函数
    - 激活函数: 在线性操作之后去做非线性变换,常用的激活函数有SigMoid，Tanh，ReLU
    - 损失函数: 网络优化的目标函数，训练的过程实际上就是最小化损失函数的过程
    ,常用的损失函数有：二次损失函数，交叉熵损失函数，对数似然函数
    - 优化函数:
    优化算法功能，改善训练方式，最大或最小化损失函数,常见的优化函数有：梯度下降（GD），自适应学习率优化算法（Adagrad，Adam）
